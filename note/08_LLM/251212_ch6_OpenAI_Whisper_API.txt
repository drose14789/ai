1. 환경 설정
Python 코드에서 python-dotenv로 .env 파일을 불러온 뒤, openai.OpenAI() 클래스를 이용해 
API 클라이언트 인스턴스를 생성

2. Whisper API 소개 및 기본 음성 → 텍스트 변환
OpenAI Whisper API는 두 가지 주요 **엔드포인트(endpoint)**를 제공
- Transcriptions 엔드포인트: 제공한 음성 파일을 해당 음성의 원어로 필사(글로 변환)
(예: 한국어 음성을 입력하면 한국어 텍스트로 출력).
- Translations 엔드포인트: 제공한 음성 파일을 영어로 번역하여 텍스트로 반환

3. 다양한 언어의 음성 인식 및 언어감지
Whisper 모델은 다국어 음성 인식을 지원. 90개가 넘는 언어로 훈련되어 다양한 언어의 음성을 
텍스트로 변환할 수 있음. 기본적으로 API에 언어를 명시하지 않으면 Whisper가 음성의 언어를 
자동으로 감지하여 해당 언어로 텍스트를 필사. 
예를 들어, 한국어 음성을 입력하면 자동으로 한국어로 텍스트를 출력

4. 고급 기능 1: 타임스탬프 포함 자막 및 세부 출력
기본적인 Whisper API 응답은 전체 음성을 하나의 텍스트로 반환하지만, 경우에 따라 문장별로 시간 
구간을 표시하거나 단어별 타임스탬프가 필요한 경우도 있음 
(예: 동영상 자막 생성, 정교한 음성 분석 등). Whisper API는 이러한 요구에 맞게 여러 가지 출력 
형식을 제공

다양한 출력 형식 (response_format 옵션)
response_format 파라미터를 조정하여 Whisper의 출력 형식을 바꿀 수 있음:

- "json" (기본값): { "text": "..." } 형태의 JSON 응답. 추가 메타정보 없이 최종 텍스트만 포함.
- "text": 순수 텍스트로 변환 결과만 반환. (이미 앞서 사용)
- "srt": SubRip 자막 형식으로 반환. 자막처럼 각 문장에 시간 코드가 포함.
- "vtt": WebVTT 자막 형식으로 반환. (srt와 유사한 자막 표준 형식)
- "verbose_json": 인식 결과에 대한 상세 정보를 담은 JSON으로 반환. 인식된 문장별 세그먼트와 
타임스탬프, 확률 등 메타데이터가 모두 포함.

자막 형식 출력: 예를 들어 음성 파일을 response_format="srt"으로 요청하면, Whisper는 
출력 텍스트를 일정 길이로 나누어 각 부분에 시작-끝 시간이 포함된 자막 형식으로 돌려줌

5. 고급 기능 2: 실시간 음성 변환 (마이크 입력 처리)
실시간 처리란 말 그대로 사용자가 말하는 동시에(text) 바로바로 텍스트로 변환하는 것을 의미. 
Whisper API는 스트리밍 엔드포인트를 제공하지는 않으나, 짧은 구간으로 녹음하여 연속으로 
API에 보내는 방식으로 유사 실시간 처리가 가능


