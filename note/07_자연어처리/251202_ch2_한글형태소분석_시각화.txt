6. 워드 임베딩
- 워드 임베딩 (Word Embedding): 단어를 벡터로 표현하는 대표적인 방법으로
  주로 희소 표현에서 밀집 표현으로 변환하는 것을 의미
- 희소 표현 (Sparse Representation)
    - 원 핫 인코딩을 통해서 나온 원 핫 벡터들은 표현하고자 하는 단어의 인덱스의 값만
      1 이고 , 나머지 인덱스에는 전부 0 으로 표현되는 벡터 표현 방법
    - 벡터 또는 행렬 ( 의 값이 대부분이 0 으로 표현되는 방법
    - 원 핫 벡터는 희소 벡터 (Sparse Vector)
- 밀집 표현(Dense Representation)
    - 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤
    - 이 과정에서 더 이상 0 과 1 만 가진 값이 아니라 실수값을 가지게 됨
    - 벡터의 차원이 조밀해졌다고 하여 밀집 벡터 (Dense Vector) 라고 함
- 워드 임베딩 (Word Embedding)
    - 단어를 밀집 벡터 (Dense Vector) 의 형태로 표현하는 방법
    - 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터 (Embedding
       Vector) 이라고 함
    - 단어간 벡터 계산 ->단어간 거리 유사성 도출
Word2Vec
- 단어 간 유사성을 고려하기 위해서는 단어의 의미를 벡터화하기 위한
  패키지
- 희소 표현 (Sparse Representation)
    - 단어간 유사성을 표현할 수 없다는 단점
- 분산 표현 (Distributed Representation)
    - 단어의 의미 를 다차원 공간에 벡터화하는 방법
    - 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다 라는 가정
    - 분산 표현을 이용하여 단어의 의미를 벡터화하는 작업은 워드 임베딩 (Embedding)
      작업에 속함
- CBOW(Continuous Bag of Words)
    - 주변에 있는 단어들을 가지고 , 중간에 있는 단어들을 예측하는 방법
- Skip Gram
    - 중간에 있는 단어로 주변 단어들을 예측하는 방법
class Word2Vec (sentences=None, size =100 window =5,
min_count =5, workers =3)
    - size : 특징 벡터의 차원
    - window : 문장 내 현재 단어와 예측 단어 사이의 최대 거리
    - min_count : 총 빈도가 이보다 낮은 모든 단어를 무시함
    - workers : 모델을 훈련시킬 쓰레드의 수
    - https://www.pydoc.io/pypi/gensim 3.2.0/autoapi/models/word2vec/index.html#models.word2vec.Word2Vec
- 패키지 설치
    - pip install gensim