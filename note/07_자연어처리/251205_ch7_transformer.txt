1. 순환신경망
순환신경망 개요
- 순환신경망은 1986년 데이비드 루멜하트(David Rumelhart)가 개발한 알고리즘
- 순환신경망은 시계열 데이터를 학습하는 딥러닝 기술
- 순환신경망은 기준 시점(t)과 다음 시점(t+1)에 네트워크를 연결함
- 순환신경망은 AI 번역, 음성인식, 주가 예측의 대표적 기술임

RNN 종류
- One to Many : 입력이 하나이고 출력이 여러 개 생성
    - One to Many는 영상에 캡션을 달아야 할 때 사용
- Many to One : 여러 입력이며, 하나의 출력을 생성
    - 영화평을 분류할 경우 Many to One이 사용
- Many to Many : 여러 입력이며, 출력도 여러 개 생성
    - 3번째 Many to Many 구조는 번역에 사용

Bidirectional RNN
- 양방향 RNN은 두 개의 RNN을 하나로 묶음
- 하나는 순방향으로 가중치를 수정하며, 다른 하나는 역방향으로 가중치를 수정함
    - RNN 단어가 길면 과거의 일을 잊는 것을 방지하고 또한 미래의 사실을 
      과거에 반영할 수 있는 구조
2. LSTM과 GRU
LSTM
- RNN의 단기 기억(Short-Term Memory) 문제를 해결하기 위해 만들어진 것
- 최근의 기억은 유지하지만 오래된 기억은 전달되지 않는 문제를 해결하고자 하는 것

GRU
- GRU(Gated Recurrent Unit): LSTM의 게이트를 단순화시켜 속도를 빠르게 개선한 것

3. Seq2Seq
Seq2Seq
- GNMT(Google Neural Machine Translation)
    - 인코더 입력, 디코더 입력, 그리고 디코더 출력으로 나눠짐

GNMT
- 인코더는 차례로 입력된 문장들의 모든 단어를 압축하여 컨텍스트 벡터를 생성
- 컨텍스트 벡터는 차대로 입력된 문장의 모든 단어의 정보를 압축한 벡터이며 
  잠재 벡터(Latent Vector)라고도 함
- 디코더는 입력된 컨텍스트 벡터를 이용하여 출력 시퀀스를 생성하고 출력

4. 어텐션
Seq2Seq의 문제점
- 인코더가 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 일부 정보 손실
    - RNN 구조의 근본적인 문제점
    - 경사 소실(Vanishing Gradient)이 발생할 가능성이 있음
    - 입력 데이터의 길이가 길어지면 성능이 저하되는 현상이 발생할 가능성이 있음

어텐션
- 어텐션 함수는 Q(Query), K(Keys), V(Values)를 매개변수로 사용
    - Q(Query)는 특정 시점에서의 디코더 셀의 은닉 상태
    - K(Key)는 모든 시점에서의 인코더 셀의 Q를 반영하기 전 은닉 상태
    - V(Value)는 모든 시점에서의 인코더 셀의 Q 반영 후 은닉 상태
        - Attention Value = Attention(Q, K, V)
    - 어텐션 함수는 주어진 질의(Query)에 대해서 모든 키(Key)와 각각의 유사도를 계산
    - 계산된 유사도를 키와 매핑되어 있는 각각의 값(Value)에 반영
    - 유사도가 반영된 값(Value)을 모두 어텐션 값(Attention Value)으로 반환
