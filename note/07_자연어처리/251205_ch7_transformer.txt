1. 순환신경망
순환신경망 개요
- 순환신경망은 1986년 데이비드 루멜하트(David Rumelhart)가 개발한 알고리즘
- 순환신경망은 시계열 데이터를 학습하는 딥러닝 기술
- 순환신경망은 기준 시점(t)과 다음 시점(t+1)에 네트워크를 연결함
- 순환신경망은 AI 번역, 음성인식, 주가 예측의 대표적 기술임

RNN 종류
- One to Many : 입력이 하나이고 출력이 여러 개 생성
    - One to Many는 영상에 캡션을 달아야 할 때 사용
- Many to One : 여러 입력이며, 하나의 출력을 생성
    - 영화평을 분류할 경우 Many to One이 사용
- Many to Many : 여러 입력이며, 출력도 여러 개 생성
    - 3번째 Many to Many 구조는 번역에 사용

Bidirectional RNN
- 양방향 RNN은 두 개의 RNN을 하나로 묶음
- 하나는 순방향으로 가중치를 수정하며, 다른 하나는 역방향으로 가중치를 수정함
    - RNN 단어가 길면 과거의 일을 잊는 것을 방지하고 또한 미래의 사실을 
      과거에 반영할 수 있는 구조
2. LSTM과 GRU
LSTM
- RNN의 단기 기억(Short-Term Memory) 문제를 해결하기 위해 만들어진 것
- 최근의 기억은 유지하지만 오래된 기억은 전달되지 않는 문제를 해결하고자 하는 것

GRU
- GRU(Gated Recurrent Unit): LSTM의 게이트를 단순화시켜 속도를 빠르게 개선한 것

3. Seq2Seq
Seq2Seq
- GNMT(Google Neural Machine Translation)
    - 인코더 입력, 디코더 입력, 그리고 디코더 출력으로 나눠짐

GNMT
- 인코더는 차례로 입력된 문장들의 모든 단어를 압축하여 컨텍스트 벡터를 생성
- 컨텍스트 벡터는 차대로 입력된 문장의 모든 단어의 정보를 압축한 벡터이며 
  잠재 벡터(Latent Vector)라고도 함
- 디코더는 입력된 컨텍스트 벡터를 이용하여 출력 시퀀스를 생성하고 출력

4. 어텐션
Seq2Seq의 문제점
- 인코더가 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 일부 정보 손실
    - RNN 구조의 근본적인 문제점
    - 경사 소실(Vanishing Gradient)이 발생할 가능성이 있음
    - 입력 데이터의 길이가 길어지면 성능이 저하되는 현상이 발생할 가능성이 있음

어텐션
- 어텐션 함수는 Q(Query), K(Keys), V(Values)를 매개변수로 사용
    - Q(Query)는 특정 시점에서의 디코더 셀의 은닉 상태
    - K(Key)는 모든 시점에서의 인코더 셀의 Q를 반영하기 전 은닉 상태
    - V(Value)는 모든 시점에서의 인코더 셀의 Q 반영 후 은닉 상태
        - Attention Value = Attention(Q, K, V)
    - 어텐션 함수는 주어진 질의(Query)에 대해서 모든 키(Key)와 각각의 유사도를 계산
    - 계산된 유사도를 키와 매핑되어 있는 각각의 값(Value)에 반영
    - 유사도가 반영된 값(Value)을 모두 어텐션 값(Attention Value)으로 반환

Transformer
- GPT 모델은 'Transformer(트랜스포머)'라는 아키텍처를 기반으로 함
    - 트랜스포머 모델은 자연어 처리와 같은 순차적인 데이터를 처리하기 위해 설계된 딥러닝 모델.
    - 2017년 구글이 “Attention is all you need” 논문에서 발표한 모델(https://arxiv.org/abs/1706.03762)
- 입력 처리: 문장을 단어 단위로 쪼개어 각 단어를 수치로 변환(임베딩)합니다. 
   이 변환된 숫자 벡터는 모델이 이해할 수 있는 형태로, 각 단어의 의미를 담고 있음.
- 어텐션 메커니즘 (Attention Mechanism): 트랜스포머의 핵심 아이디어는 어텐션 메커니즘. 
  어텐션은 입력의 모든 단어가 서로 얼마나 중요한지를 계산하여, 
  중요한 단어에 더 집중할 수 있도록 함. 문장 내에서 어떤 단어들이 서로 관련이 있는지를 판단하고,
  그 연관성을 바탕으로 정보의 흐름을 조절함.
- 인코더와 디코더: 트랜스포머는 인코더와 디코더로 구성.
    - 인코더: 입력 문장을 받아 어텐션을 통해 의미 있는 벡터로 변환.
    - 디코더: 인코더에서 얻은 벡터를 받아 새로운 문장을 생성하거나, 번역 작업 등을 수행.
- 병렬 처리: 트랜스포머는 RNN이나 LSTM과 달리 모든 단어를 한 번에 처리할 수 있어 학습이 빠름. 
  이는 어텐션 메커니즘 덕분에 가능하며, 이전 단어들을 순차적으로 보지 않아도 
  각 단어의 중요도를 빠르게 계산할 수 있음.

트랜스포머의 인코더와 디코더 구조
- 인코딩 컴포넌트는 여러 인코더로 구성되어 있음
- 디코딩 컴포넌트는 인코딩 컴포넌트와 같은 개수의 디코더로 구성
    - 논문에는 6개의 인코더와 디코더로 구성되어 있으나 임의의 개수로 변경 가능

트랜스포머의 임베딩과 Positional 인코딩
- Positional 인코딩은 트랜스포머에서 각 입력 단어의 위치 정보 부여를 위해 
  각 단어의 임베딩 벡터에 위치 정보들을 추가하여 모델의 입력으로 사용하는 방법

트랜스포머의 Self-Attention
- 트랜스포머의 Self-Attention은 현재 처리 중인 단어에 대해 다른 연관 있는 
  단어들과의 맥락을 파악하는 방법을 제공
- Self-Attention의 첫 단계는 입력 문장에 대해 Query, Key, Value를 계산하는 것
- 입력 문장의 512 크기의 벡터와 학습할 가중치(WQ, WK, WV)를 곱하여 64 크기의
  Query, Key, Value 벡터를 생성
Query에 Key의 전치(Transpose) 행렬을 곱해서 내적을 계산
- 만일 Query와 Key가 특정 문장에서 중요한 역할을 하고 있다면 트랜스포머는 
  이들 사이의 내적(Dot Product)값을 크게 하는 방향으로 학습
- 내적 값이 커지면 해당 Query와 Key가 벡터 공간상 가까이 있을 확률이 높음
- Key의 벡터 크기인 64의 제곱근인 8로 나눈 후 소프트맥스 함수를 적용
- Value와 내적을 곱하여 어텐션 값인 Z를 계산

Multi-head Self Attention
- Attention을 병렬로 수행함으로써 다양한 관점에서 단어 간 관계 정보 파악이 가능
- Input 문장에 대해 N개의 Query, Key, Value를 계산하기 위한 N개의 가중치(WQ, WK, WV)를 
  생성하여 병렬로 Self Attention 수행하고 N개의 Attention Value를 계산

Multi-head Attention Value Matrix
- Multi-head Attention 수행 결과를 연결하고 학습할 가중치를 곱하여 
  Multi-head Attention Value Matrix를 최종 결과로 도출

Position-wise FFN
- Position-wise FFN은 인코더와 디코더에서 공통으로 포함된 Sub-layer

잔차연결
- 경사가 줄어드는 문제를 해결하기 위해 구글은 ResNet을 통해 잔차연결을 적용

정규화
- Residual Connection으로 전달된 입력값과 Multi-head Attention Value를 더한 후 
  정규화(Normalization)를 수행
- Position-wise FFNN의 입력값으로 전달하기 전 정규화를 수행함으로써 
  과적합(Over-fitting)을 방지할 수 있음