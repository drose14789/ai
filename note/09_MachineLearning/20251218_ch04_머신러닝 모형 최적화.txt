1절. 변수 선택과 차원 축소

1-1 변수선택과 차원축소
- 종속변수에 영향을 주는 변수들을 찾아 학습에 사용할 독립변수의 수를 줄임
 (어떻게 하면 score를 높일 수 있을지?)
- 과적합과 변수들 사이의 다중공선성(변수들간 강한 상관관계)을 줄일 수 있음
 * 회귀계수 해석이 어려워짐. 모델 예측력이 좋아도 해석력이 떨어짐
    (어떤 변수가 제일 큰 요인인지 잘), p값이나 유의성 검정이 왜곡될 수 있음
- 모형의 학습 시간을 줄일 수 있음
- 주성분분석, 상관분석, **분류모형의 feature_importance_, 예측 모형의 coef_**
- SelectKBest : 가장 높은 score에 따라 K개의 특징을 선택

1-2 주성분분석(PCA, Principal Component Anaysis)
- 주성분분석은 변수 선택 및 차원축소 방법(기존의 모든 변수를 조합하여 새로운 변수로 만듦) 으로 
  널리 사용
- 주성분 분석은 상관관계가 있는 변수들을 선형결합해서 **분산이 극대화된 상관관계가 없는 새로운
 변수(주성분)들로 축약**하는 것
- 주성분 분석은 사실 선형대수학이라기보다는 선형대수학의 활용적인 측면이 강하며 영상인식, 통계
 데이터분석(주성분 찾기), 데이터 압축, 노이즈제거 등 여러 분야에 사용
- 영상처리에서 많이 활용 : 여러개의 영상 중 대표 이미지를 찾을 때 활용

1-3 상관관계 확인
- 각 변수들끼리 상관관계를 확인하고 시각화해서 종속변수와 상관관계가 높은 변수들만 선택

1-4 분류모형의 Feature Importance
- 분류모형의 featur_importance_ 속성 : 각 독립변수들이 종속변수에 영향을 주는 정도
- DecisionTreeClassifier, RandomForestClassifer, GradientBoostingClassifier..(tree계열)
- LogisticRegression, SVC, MLPclassifier 등은 feature_importance_ 속성없음

feature_importances_를 이용한 변수 중요도 시각화

RFE(Recursive Feature Elimination) 방식¶
- RFE를 이용하면 중요도에 따라 중요도가 낮은 변수부터 하나씩 제거해 나가면서 최종적으로 
  선택한 변수를 찾는다

1-5 SelectKBest
- 가장 높은 score에 따라 k개 특징변수 선택

2절. 파라미터 탐색
- 하이퍼파라미터(모델의 성능에 영향을 미칠 수 있는 사용자가 직접 설정하는 파라미터). 
- 어떤 파라미터를 사용하는게 최적의 결과를 낼지 탐색
- sklearn패키지의 하이퍼 파라미터 튜닝 도구
    * validation_curve(): 단일 하이퍼 파라미터 최적화 함수
    * GridSearchCV : 복수 하이퍼파라미터 최적화 클래스

2-1 validation_curve
    - 모형, x, y, param_name(파라미터이름), param_range(파라미터값list), 교차검증

2-2 GridSearchCV
- 복수 하이퍼파라미터 최적화 클래스
- 모형 클래스를 가지고 있음
- fit()/predict()/ score() / predict_porba() / predict_log_proba()

3절 자료 불균형 처리
- 단순 오버/언더 샘플링
- 단, 단순 오버샘플시 소스의 데이터를 복사하면 그 데이터들에 의해 과적합 생길 수 있음
- SMOTE 라이브러리를 이용한 오버샘플링

3-1 SMOTE를 이용한 오버 샘플링

3-2 가중치 제어 모형
- 자료 불균형 처리의 또 다른 방법
- 모델에 데이터 따른 가중치 부여 방법

4절. 앙상블 모형
- 목적 : 여러개 분류 모델을 하나의 통합 분류모델로 연견하여 개별 분류모델보다 더 좋은 성능 달성
- 방법 :
    * 배깅(bagging) : 분류를 잘하는 모델에 가중치 (병렬작업) ex.RadomForest
    * 부스팅(boosting) : 분류가 안된 데이터에 가중치(순차작업) ex. XGBoost, LGBM, 
		      AdaBoost-불균형데이터
    * 투표(Voting) : 여러개 모델의 다수결 투표

4-1 배깅
- RandomForestClassifier, BaggingClassifier